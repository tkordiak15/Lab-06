[
  {
    "objectID": "lab-06.html",
    "href": "lab-06.html",
    "title": "Lab-06",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\nlibrary(baguette)\n\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quite = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n#the zero_q_freq from the camels attribute means the frequency of days with stream flow equals zero mm per day that is in units of percentages and is from the N15 - USGS data set.\n\ncamels &lt;- power_full_join(camels ,by = \"gauge_id\")\n\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") + \n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") + \n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-06.html#question-one",
    "href": "lab-06.html#question-one",
    "title": "Lab-06",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\nlibrary(baguette)\n\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quite = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n#the zero_q_freq from the camels attribute means the frequency of days with stream flow equals zero mm per day that is in units of percentages and is from the N15 - USGS data set.\n\ncamels &lt;- power_full_join(camels ,by = \"gauge_id\")\n\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") + \n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") + \n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-06.html#question-two",
    "href": "lab-06.html#question-two",
    "title": "Lab-06",
    "section": "Question Two",
    "text": "Question Two\n\ncamels |&gt;\n  select(aridity, p_mean, q_mean) |&gt;\n  drop_na() |&gt;\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nprint(camels)\n\n# A tibble: 671 × 58\n   gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 01013500   3.13     1.97        0.188      0.313   0.631           13.0\n 2 01022500   3.61     2.12       -0.115      0.245   0.587           20.6\n 3 01030500   3.27     2.04        0.0474     0.277   0.624           17.2\n 4 01031500   3.52     2.07        0.104      0.292   0.588           18.9\n 5 01047000   3.32     2.09        0.148      0.280   0.629           20.1\n 6 01052500   3.73     2.10        0.152      0.353   0.562           13.5\n 7 01054200   4.07     2.13        0.105      0.300   0.523           17.5\n 8 01055000   3.49     2.09        0.167      0.306   0.599           19.2\n 9 01057000   3.57     2.13        0.0791     0.251   0.597           20.4\n10 01073000   3.50     2.21        0.0304     0.175   0.630           20.8\n# ℹ 661 more rows\n# ℹ 51 more variables: high_prec_dur &lt;dbl&gt;, high_prec_timing &lt;chr&gt;,\n#   low_prec_freq &lt;dbl&gt;, low_prec_dur &lt;dbl&gt;, low_prec_timing &lt;chr&gt;,\n#   geol_1st_class &lt;chr&gt;, glim_1st_class_frac &lt;dbl&gt;, geol_2nd_class &lt;chr&gt;,\n#   glim_2nd_class_frac &lt;dbl&gt;, carbonate_rocks_frac &lt;dbl&gt;, geol_porostiy &lt;dbl&gt;,\n#   geol_permeability &lt;dbl&gt;, soil_depth_pelletier &lt;dbl&gt;,\n#   soil_depth_statsgo &lt;dbl&gt;, soil_porosity &lt;dbl&gt;, soil_conductivity &lt;dbl&gt;, …\n\n\n#scatterplot\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n              scale_color_viridis_c() +\n              theme_linedraw() + theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runoff\",\n       x = \"aridity\",\n       y = \"rainfall\",\n       color = \"mean flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#transformation\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#another transformation\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#model building\n\nset.seed(123)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n#preprocessor\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n#Naive approach\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab-06.html#question-three",
    "href": "lab-06.html#question-three",
    "title": "Lab-06",
    "section": "Question Three",
    "text": "Question Three\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\n\nxgb_model &lt;- boost_tree(trees = 1000,\n                        tree_depth = 6,\n                        learn_rate = 0.01) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\n#Neutral network model\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\", times = 10) %&gt;%\n  set_mode(\"regression\")\n\nnn_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nn_model) %&gt;%\n  fit(data = camels_train)\n\n#evaluate\n\nxgb_data &lt;- augment(xgb_workflow, new_data = camels_test)\nnn_data &lt;- augment(nn_workflow, new_data = camels_test)\n\nmetrics(xgb_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.679\n2 rsq     standard       0.667\n3 mae     standard       0.413\n\nmetrics(nn_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.552\n2 rsq     standard       0.768\n3 mae     standard       0.343\n\n\n#compare\n\nwf &lt;- workflow_set(\n  list(rec),\n  list(lm_model, rf_model, xgb_model, nn_model)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.559  0.0282    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0220    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.564  0.0252    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0259    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.625  0.0256    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.728  0.0290    10 recipe       boos…     4"
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n#loading data\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quite = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\ncamel &lt;- power_full_join(camels ,by = \"gauge_id\") %&gt;% mutate(gauge_id = as.numeric(gauge_id))\n\n#libraries\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(skimr)\nlibrary(visdat)\nlibrary(ggpubr)\n\n\ncamels_clean2 &lt;- camel %&gt;%\n  filter(complete.cases(select(., aridity, p_mean, q_mean))) %&gt;%\n  select(aridity, p_mean, q_mean, gauge_lat, gauge_lon)\n\n\nskim(camels_clean2)\n\n\nData summary\n\n\nName\ncamels_clean2\n\n\nNumber of rows\n670\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naridity\n0\n1\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\np_mean\n0\n1\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\nq_mean\n0\n1\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1\n39.25\n5.21\n27.05\n35.69\n39.26\n43.23\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1\n-95.81\n16.22\n-124.39\n-110.41\n-92.84\n-81.76\n-67.94\n▆▃▇▇▅\n\n\n\n\nvis_miss(camels_clean2)\n\n\n\n\n\n\n\n\n#data splitting\n\nlibrary(rsample)\nset.seed(123)\n\n\ncamels_split &lt;- initial_split(camels_clean2, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test &lt;- testing(camels_split)\n\n\ndim(camels_train)\n\n[1] 536   5\n\ndim(camels_test)\n\n[1] 134   5\n\n\n#feature engineering and removing gauge_lon and lat\n\nlibrary(recipes)\ncamels_recipe2 &lt;- recipe(q_mean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_numeric_predictors(), base = 10) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n\nsummary(camels_train)\n\n    aridity           p_mean           q_mean           gauge_lat    \n Min.   :0.2203   Min.   :0.6446   Min.   :0.004553   Min.   :27.05  \n 1st Qu.:0.6948   1st Qu.:2.3964   1st Qu.:0.629278   1st Qu.:35.49  \n Median :0.8509   Median :3.2411   Median :1.156617   Median :39.12  \n Mean   :1.0471   Mean   :3.2944   Mean   :1.523382   Mean   :39.14  \n 3rd Qu.:1.2188   3rd Qu.:3.8263   3rd Qu.:1.798760   3rd Qu.:43.15  \n Max.   :5.2079   Max.   :8.7924   Max.   :9.688438   Max.   :48.66  \n   gauge_lon      \n Min.   :-124.39  \n 1st Qu.:-110.69  \n Median : -92.78  \n Mean   : -95.81  \n 3rd Qu.: -81.62  \n Max.   : -67.94  \n\n\n##resamples and model testing #build 3 candidates models\n\nlibrary(parsnip)\nlibrary(rsample)\ncamels_cv &lt;- vfold_cv(camels_train, v = 10, strata = \"q_mean\")\n\n\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlinear_mod &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nrf_mod &lt;- rand_forest(mtry = 2, trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nboosted_mod &lt;- boost_tree(trees = 1000,\n                        tree_depth = 6,\n                        min_n = 10) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n#test the models\n\nlibrary(workflows)\nlibrary(workflowsets)\nlibrary(tune)\nlibrary(yardstick)\n\nmodel_workflows &lt;- workflow_set(\n  preproc = list(camels_recipe2),\n  models = list(\n    linear = linear_mod,\n    random_forest = rf_mod,\n    xgboost = boosted_mod\n  )\n)\n\n\nmodel_results &lt;- model_workflows %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\n\nautoplot(model_results)\n\n\n\n\n\n\n\n\n\ncollect_metrics(model_results)\n\n# A tibble: 6 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_linear     Prepro… recipe  line… rmse    standard   0.939    10  0.0572\n2 recipe_linear     Prepro… recipe  line… rsq     standard   0.654    10  0.0193\n3 recipe_random_fo… Prepro… recipe  rand… rmse    standard   0.524    10  0.0424\n4 recipe_random_fo… Prepro… recipe  rand… rsq     standard   0.880    10  0.0230\n5 recipe_xgboost    Prepro… recipe  boos… rmse    standard   0.645    10  0.0461\n6 recipe_xgboost    Prepro… recipe  boos… rsq     standard   0.839    10  0.0204\n\n\n#Model selection #I will be using the random forest, the reason I’m picking it is because it is highly ranked within the graph and the lowest rmse (0.51) and the highest rsq (0.89) which is what we are looking for in best performing models. # The random forest model helps when there are lots of different trees and complies one with averaged results. the engine “ranger” and in “regression” mode works because there is an already strong correlation to q-mean and the predictors.\n##Model Tuning\n\nrf_model_tune &lt;- rand_forest(\n  mode = \"regression\",\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(dials)\n\n#check turntable values/ranges and create workflow\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 5)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\nrf_grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     1     2\n 2     2     2\n 3     3     2\n 4     4     2\n 5     5     2\n 6     1     4\n 7     2     4\n 8     3     4\n 9     4     4\n10     5     4\n# ℹ 15 more rows\n\n\n#tuning 1\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(camels_recipe2) %&gt;%\n  add_model(rf_model_tune)\n\nrf_tune_results &lt;- rf_workflow %&gt;%\n  tune_grid(\n    resamples = camels_cv,\n    grid = rf_grid\n  )\n\n→ A | warning: ! 3 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! 4 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n→ C | warning: ! 5 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x2   B: x1   C: x1\nThere were issues with some computations   A: x4   B: x4   C: x4\nThere were issues with some computations   A: x6   B: x6   C: x6\nThere were issues with some computations   A: x8   B: x8   C: x8\nThere were issues with some computations   A: x11   B: x10   C: x10\nThere were issues with some computations   A: x13   B: x12   C: x12\nThere were issues with some computations   A: x15   B: x15   C: x14\nThere were issues with some computations   A: x17   B: x17   C: x16\nThere were issues with some computations   A: x19   B: x19   C: x18\nThere were issues with some computations   A: x21   B: x21   C: x20\nThere were issues with some computations   A: x23   B: x22   C: x22\nThere were issues with some computations   A: x25   B: x25   C: x24\nThere were issues with some computations   A: x27   B: x26   C: x26\nThere were issues with some computations   A: x29   B: x28   C: x28\nThere were issues with some computations   A: x31   B: x31   C: x30\nThere were issues with some computations   A: x33   B: x33   C: x32\nThere were issues with some computations   A: x35   B: x35   C: x35\nThere were issues with some computations   A: x37   B: x36   C: x36\nThere were issues with some computations   A: x39   B: x39   C: x38\nThere were issues with some computations   A: x41   B: x41   C: x40\nThere were issues with some computations   A: x43   B: x43   C: x42\nThere were issues with some computations   A: x46   B: x45   C: x45\nThere were issues with some computations   A: x47   B: x47   C: x46\nThere were issues with some computations   A: x49   B: x49   C: x49\nThere were issues with some computations   A: x50   B: x50   C: x50\n\nautoplot(rf_tune_results)\n\n\n\n\n\n\n\n\n#tuning 2\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(camels_recipe2) %&gt;%\n  add_model(rf_model_tune)\n\nrf_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: ranger \n\n\n#define the search space/dials\n\nlibrary(dials)\n\ndials &lt;- extract_parameter_set_dials(rf_workflow)\n\ndials &lt;- update(dials, mtry = mtry(range = c(1, 10)))\n\ndials &lt;- update(dials, min_n = min_n(range = c(2, 15)))\n\nmy_grid &lt;- grid_space_filling(dials, size = 25)\n\ndials\n\nCollection of 2 parameters for tuning\n\n\n\n\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      min_n min_n nparam[+]\n\n\n\n\n\n\ndials &lt;- extract_parameter_set_dials(rf_workflow)\ndials\n\nCollection of 2 parameters for tuning\n\n\n\n\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\n\n\n\n\nModel parameters needing finalization:\n\n\n# Randomly Selected Predictors ('mtry')\n\n\n\n\n\nSee `?dials::finalize()` or `?dials::update.parameters()` for more information.\n\n\n\ndials$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, ?]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n\n#4\n\ndials &lt;- extract_parameter_set_dials(rf_workflow)\n\ndials &lt;- finalize(dials, camels_train)\n\nmy.grid &lt;- grid_space_filling(dials, size = 25)\n\nmy.grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     1    24\n 2     1    13\n 3     1    33\n 4     1     5\n 5     1    19\n 6     2    27\n 7     2    36\n 8     2    11\n 9     2     3\n10     2    21\n# ℹ 15 more rows\n\n\n#tuning my model\n\nmodel_params &lt;- tune_grid(\n  rf_workflow,\n  resamples = camels_cv,\n  grid = my.grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\n→ A | warning: ! 3 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\n→ B | warning: ! 4 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\n\n\nThere were issues with some computations   A: x3\n→ C | warning: ! 5 columns were requested but there were 2 predictors in the data.\n               ℹ 2 predictors will be used.\nThere were issues with some computations   A: x3\nThere were issues with some computations   A: x6   B: x5   C: x5\nThere were issues with some computations   A: x10   B: x5   C: x5\nThere were issues with some computations   A: x11   B: x10   C: x10\nThere were issues with some computations   A: x15   B: x12   C: x10\nThere were issues with some computations   A: x16   B: x15   C: x15\nThere were issues with some computations   A: x20   B: x18   C: x15\nThere were issues with some computations   A: x21   B: x20   C: x20\nThere were issues with some computations   A: x25   B: x25   C: x20\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x30   B: x30   C: x27\nThere were issues with some computations   A: x31   B: x30   C: x30\nThere were issues with some computations   A: x35   B: x35   C: x33\nThere were issues with some computations   A: x38   B: x35   C: x35\nThere were issues with some computations   A: x40   B: x40   C: x40\nThere were issues with some computations   A: x44   B: x40   C: x40\nThere were issues with some computations   A: x45   B: x45   C: x45\nThere were issues with some computations   A: x50   B: x45   C: x45\nThere were issues with some computations   A: x50   B: x50   C: x50\n\n\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\nI can see six different graphs, one shows the rmse, rsq, and mae with randomly selected predictors and minimal node size as the x-axis. mae and rmse are failry similar with no outliers which is what we’re looking for. proving that the model was a good choice because of less outliers and consistancy.\n#Check the skill of the tuned model\n\nmodel_params %&gt;% collect_metrics()\n\n# A tibble: 75 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    24 mae     standard   0.323    10  0.0153 Preprocessor1_Model01\n 2     1    24 rmse    standard   0.504    10  0.0359 Preprocessor1_Model01\n 3     1    24 rsq     standard   0.891    10  0.0219 Preprocessor1_Model01\n 4     1    13 mae     standard   0.319    10  0.0154 Preprocessor1_Model02\n 5     1    13 rmse    standard   0.502    10  0.0348 Preprocessor1_Model02\n 6     1    13 rsq     standard   0.890    10  0.0222 Preprocessor1_Model02\n 7     1    33 mae     standard   0.325    10  0.0155 Preprocessor1_Model03\n 8     1    33 rmse    standard   0.509    10  0.0397 Preprocessor1_Model03\n 9     1    33 rsq     standard   0.893    10  0.0211 Preprocessor1_Model03\n10     1     5 mae     standard   0.320    10  0.0170 Preprocessor1_Model04\n# ℹ 65 more rows\n\nmodel_params %&gt;%\n  show_best(metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     1    13 mae     standard   0.319    10  0.0154 Preprocessor1_Model02\n2     1     5 mae     standard   0.320    10  0.0170 Preprocessor1_Model04\n3     5    32 mae     standard   0.322    10  0.0173 Preprocessor1_Model25\n4     2    11 mae     standard   0.322    10  0.0156 Preprocessor1_Model08\n5     1    19 mae     standard   0.323    10  0.0154 Preprocessor1_Model05\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\nhp_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     1    13 Preprocessor1_Model02\n\n\n\n\nthe first row shows the best performing model has a mtry = 5 and a min_n = 22, meaning the model selected 5 random predictors and needed 10 data points to split a node, with an average mean of 0.312 and our predictions are off by that much. lower means leads to a better performing model.\n#finalized model\n\nfinal_workflow &lt;- finalize_workflow(rf_workflow, hp_best)\n\nprint(final_workflow)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 1\n  min_n = 13\n\nComputational engine: ranger \n\nfinal_fit &lt;- last_fit(final_workflow, camels_split)\n\nfinal_metrics &lt;- collect_metrics(final_fit)\nfinal_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.401 Preprocessor1_Model1\n2 rsq     standard       0.920 Preprocessor1_Model1\n\n\n\n\nthe results show a dinal rmse of 0.401 and a rsq of 0.920. Lower rmse means that the predicted values are close to actual values and a high rsq means that 92% of varablitly in the test data, these are great results showing a strong model choice.\n\nfinal_predictions &lt;- collect_predictions(final_fit)\nfinal_predictions\n\n# A tibble: 134 × 5\n   .pred id                .row q_mean .config             \n   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;               \n 1  1.70 train/test split     1  1.70  Preprocessor1_Model1\n 2  1.83 train/test split     3  1.82  Preprocessor1_Model1\n 3  2.07 train/test split     9  1.82  Preprocessor1_Model1\n 4  1.96 train/test split    15  2.08  Preprocessor1_Model1\n 5  1.79 train/test split    18  1.81  Preprocessor1_Model1\n 6  2.11 train/test split    22  2.24  Preprocessor1_Model1\n 7  2.13 train/test split    35  1.81  Preprocessor1_Model1\n 8  1.71 train/test split    42  1.76  Preprocessor1_Model1\n 9  1.13 train/test split    43  0.853 Preprocessor1_Model1\n10  1.26 train/test split    44  0.997 Preprocessor1_Model1\n# ℹ 124 more rows\n\n\n\nlibrary(ggplot2)\n\nggplot(final_predictions, aes(x = .pred, y = q_mean)) +\n  geom_point(aes(color = \"purple\"), alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"pink\", se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"blue\")) +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Streamflow (q_mean)\",\n    y = \"Actual Streamflow (q_mean)\",\n    caption = \"Red line: Linear fit\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#map building\n\nfinal_model &lt;- fit(final_workflow, data = camels_clean2)\npredictions &lt;- augment(final_model, camels_clean2)\n\npredictions &lt;- predictions %&gt;%\n  mutate(residuals = .pred - q_mean)\n\nhead(predictions)\n\n# A tibble: 6 × 7\n  .pred aridity p_mean q_mean gauge_lat gauge_lon residuals\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  1.72   0.631   3.13   1.70      47.2     -68.6   0.0194 \n2  2.08   0.587   3.61   2.17      44.6     -67.9  -0.0967 \n3  1.84   0.624   3.27   1.82      45.5     -68.3   0.0238 \n4  2.03   0.588   3.52   2.03      45.2     -69.3  -0.00125\n5  1.91   0.629   3.32   2.18      44.9     -70.0  -0.273  \n6  2.22   0.562   3.73   2.41      44.9     -71.1  -0.186  \n\n\n#predicted map\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\npred_map &lt;- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) + borders(\"state\", colour = \"black\", fill = NA) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Predicted Streamflow (q_mean)\", color = \"Predicted Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(pred_map)\n\n\n\n\n\n\n\n\n#residual map\n\nresid_map &lt;- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = residuals)) + borders(\"state\", colour = \"black\", fill = NA) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Residuals of Predictions (Predicted - Actual)\", color = \"Residuals\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(resid_map)\n\n\n\n\n\n\n\n\n#combined maps\n\nlibrary(patchwork)\n\ncombined_map &lt;- pred_map + resid_map +\n  plot_layout(ncol = 2, heights = c(6, 6))\n\nprint(combined_map)"
  }
]